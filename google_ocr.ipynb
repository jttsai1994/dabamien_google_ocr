{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "QDd5wgVRMn8b",
        "outputId": "97bdf26a-b35c-4174-f0dc-04c6977817b4"
      },
      "outputs": [],
      "source": [
        "# pip install --upgrade google-cloud-documentai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nRsNsL_gMwqL",
        "outputId": "156204a4-79a6-4ff4-d618-1356f4569e6b"
      },
      "outputs": [],
      "source": [
        "# pip install --upgrade google-cloud-documentai-toolbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHKiEklTNCCT",
        "outputId": "eab286bc-20a5-479c-92e5-7b0e4d0a71b8"
      },
      "outputs": [],
      "source": [
        "# pip install --upgrade google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 設置環境變數，指向服務帳戶密鑰檔案\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] =  \"dabamien-2228a0999974.json\" #替換成你的檔案名稱 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID =  \"dabamien\"  #<你的project_id>\n",
        "LOCATION = \"us\"  # Format is \"us\" or \"eu\"\n",
        "PROCESSOR_ID = \"ef432a9e6acb161f\" #<你建立的你建立的processor_id>\n",
        "GCS_INPUT_URL = \"gs://bamien/hsindian\"                                   # Must end with a trailing slash `/`. Format: gs://bucket/directory/subdirectory/\n",
        "GCS_OUTPUT_URL =  \"gs://bamien/results\" #<Google Storage上儲存結果的路徑> # Must end with a trailing slash `/`. Format: gs://bucket/directory/subdirectory/\n",
        "BUCKET_NAME = \"bamien\"                 #Google Storage上的Bucket Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eUOkklDi0KF"
      },
      "outputs": [],
      "source": [
        "# for page in document_object.pages:\n",
        "#     for paragraph in page.paragraphs:\n",
        "#       start_index = paragraph.layout.text_anchor.text_segments[0].start_index\n",
        "#       end_index = paragraph.layout.text_anchor.text_segments[0].end_index\n",
        "#       paragraph_text = document_object.text[start_index:end_index]\n",
        "#       print(f\"段落: {paragraph_text}\")\n",
        "#     print(\"==========\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "27WWu8DuPwsR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "\n",
        "def parse_document(document_object):\n",
        "    # 初始化結果列表\n",
        "    addresses = []\n",
        "    birth_dates = []\n",
        "    id_numbers = []\n",
        "\n",
        "    # 遍歷每一頁\n",
        "    for page_number, page in enumerate(document_object.pages, start=1):\n",
        "        # 遍歷每一段落\n",
        "        for paragraph in page.paragraphs:\n",
        "            start_index = paragraph.layout.text_anchor.text_segments[0].start_index\n",
        "            end_index = paragraph.layout.text_anchor.text_segments[0].end_index\n",
        "            paragraph_text = document_object.text[start_index:end_index].strip()\n",
        "            print(paragraph_text)\n",
        "            print(\"================\")\n",
        "\n",
        "            # 初始化當前段落的資料\n",
        "            current_data = {\n",
        "                \"地址\": None,\n",
        "                \"里\": None,\n",
        "                \"鄰\": None,\n",
        "                \"路/街\": None,\n",
        "                \"出生年月日\": None,\n",
        "                \"年齡\": None,\n",
        "                \"身分證字號\": None,\n",
        "                \"性別\": None,\n",
        "                \"頁碼\": page_number\n",
        "            }\n",
        "\n",
        "            # 提取地址\n",
        "            if \"里\" in paragraph_text and \"鄰\" in paragraph_text:\n",
        "                current_data[\"地址\"] = paragraph_text\n",
        "                current_data[\"里\"] = extract_specific_field(paragraph_text, r\"([一-龥]{1,2})里\")\n",
        "                current_data[\"鄰\"] = extract_specific_field(paragraph_text, r\"(\\d{1,2})鄰\")\n",
        "                current_data[\"路/街\"] = extract_specific_field(paragraph_text, r\"([一-龥]{1,3})(路|街)(?!鄰)\")\n",
        "\n",
        "            # 提取出生年月日\n",
        "            birth_date = extract_birth_date(paragraph_text)\n",
        "            if birth_date:\n",
        "                current_data[\"出生年月日\"] = birth_date\n",
        "                current_data[\"年齡\"] = calculate_age(birth_date)\n",
        "\n",
        "            # 提取身分證字號\n",
        "            if re.search(r\"[A-Z][0-9]+\", paragraph_text):\n",
        "                current_data[\"身分證字號\"] = paragraph_text\n",
        "                current_data[\"性別\"] = determine_gender(paragraph_text)\n",
        "\n",
        "            # 如果該段落有地址，存入地址列表\n",
        "            if current_data[\"地址\"]:\n",
        "                addresses.append({\n",
        "                    \"地址\": current_data[\"地址\"],\n",
        "                    \"里\": current_data[\"里\"],\n",
        "                    \"鄰\": current_data[\"鄰\"],\n",
        "                    \"路/街\": current_data[\"路/街\"],\n",
        "                    \"頁碼\": current_data[\"頁碼\"]\n",
        "                })\n",
        "\n",
        "            # 如果該段落有出生年月日，存入出生年月日列表\n",
        "            if current_data[\"出生年月日\"]:\n",
        "                birth_dates.append({\n",
        "                    \"出生年月日\": current_data[\"出生年月日\"],\n",
        "                    \"年齡\": current_data[\"年齡\"],\n",
        "                    \"頁碼\": current_data[\"頁碼\"]\n",
        "                })\n",
        "\n",
        "            # 如果該段落有身分證字號，存入身分證字號列表\n",
        "            if current_data[\"身分證字號\"]:\n",
        "                id_numbers.append({\n",
        "                    \"身分證字號\": current_data[\"身分證字號\"],\n",
        "                    \"性別\": current_data[\"性別\"],\n",
        "                    \"頁碼\": current_data[\"頁碼\"]\n",
        "                })\n",
        "\n",
        "    return addresses, birth_dates, id_numbers\n",
        "\n",
        "\n",
        "def extract_names(document_object, file_name):\n",
        "    \"\"\"提取名字並記錄所在檔案和頁碼\"\"\"\n",
        "    names = []  # 用於存儲名字及其位置\n",
        "\n",
        "    # 遍歷每一頁\n",
        "    for page_number, page in enumerate(document_object.pages, start=1):\n",
        "        for paragraph in page.paragraphs:\n",
        "            start_index = paragraph.layout.text_anchor.text_segments[0].start_index\n",
        "            end_index = paragraph.layout.text_anchor.text_segments[0].end_index\n",
        "            paragraph_text = document_object.text[start_index:end_index].strip()\n",
        "\n",
        "            # 檢測是否是名字（假設名字是兩到三個中文字）\n",
        "            if re.match(r\"^[\\u4e00-\\u9fa5]{2,3}$\", paragraph_text):  # 中文姓名\n",
        "                names.append({\n",
        "                    \"檔案名稱\": file_name,\n",
        "                    \"頁碼\": page_number,\n",
        "                    \"姓名\": paragraph_text\n",
        "                })\n",
        "\n",
        "    return names\n",
        "\n",
        "\n",
        "def extract_specific_field(text, pattern):\n",
        "    \"\"\"提取地址中的特定字段（如里、鄰、路/街）\"\"\"\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        result = match.group(0)\n",
        "        return result.replace(\"鄰\", \"\")  # 確保「鄰」字被移除\n",
        "    return None\n",
        "\n",
        "def extract_birth_date(text):\n",
        "    \"\"\"提取生日，支持多種格式並修正錯誤年份\"\"\"\n",
        "    match = re.search(r\"(\\d{2,3}\\.\\d{1,2}\\.\\d{1,2})|民國(\\d{2,3})年(\\d{1,2})月(\\d{1,2})日|(\\d{2,3})年(\\d{1,2})月(\\d{1,2})日\", text)\n",
        "    if match:\n",
        "        if match.group(1):  # 簡寫格式\n",
        "            year, month, day = match.group(1).split(\".\")\n",
        "            year = correct_year(year)\n",
        "            return f\"{year}.{month}.{day}\"\n",
        "        elif match.group(2):  # 民國完整格式\n",
        "            year = correct_year(match.group(2))\n",
        "            month = match.group(3)\n",
        "            day = match.group(4)\n",
        "            return f\"{year}.{month}.{day}\"\n",
        "        elif match.group(5):  # 未標註民國的格式\n",
        "            year = correct_year(match.group(5))\n",
        "            month = match.group(6)\n",
        "            day = match.group(7)\n",
        "            return f\"{year}.{month}.{day}\"\n",
        "    return None\n",
        "\n",
        "def correct_year(year):\n",
        "    \"\"\"修正年份，確保符合規則\"\"\"\n",
        "    year = int(year)\n",
        "    if len(str(year)) == 3 and str(year)[0] != \"1\":\n",
        "        year = int(str(year)[-2:])\n",
        "    return year\n",
        "\n",
        "def calculate_age(birth_date):\n",
        "    \"\"\"根據出生年月日計算年齡\"\"\"\n",
        "    try:\n",
        "        if re.match(r\"^\\d{2,3}\\.\\d{1,2}\\.\\d{1,2}$\", birth_date):\n",
        "            year, month, day = map(int, birth_date.split(\".\"))\n",
        "            year += 1911  # 民國轉西元\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        birth_date = datetime(year, month, day)\n",
        "        today = datetime.today()\n",
        "        age = today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day))\n",
        "        return age\n",
        "    except Exception as e:\n",
        "        print(f\"無法計算年齡: {e}\")\n",
        "        return None\n",
        "\n",
        "def determine_gender(id_number):\n",
        "    \"\"\"根據身分證字號判斷性別\"\"\"\n",
        "    try:\n",
        "        match = re.search(r\"[A-Z]([0-9])\", id_number)\n",
        "        if match:\n",
        "            gender_digit = int(match.group(1))\n",
        "            return \"男性\" if gender_digit == 1 else \"女性\"\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"無法判斷性別: {e}\")\n",
        "        return None\n",
        "\n",
        "# 主程式\n",
        "def main(document_object, FILE_PATH):\n",
        "    file_name = FILE_PATH.split(\"/\")[-1]  # 提取檔案名稱\n",
        "    # 提取地址、出生年月日、身分證字號\n",
        "    addresses, birth_dates, id_numbers = parse_document(document_object)\n",
        "\n",
        "    # 提取名字\n",
        "    names = extract_names(document_object, file_name)\n",
        "\n",
        "    # 將結果存入 DataFrame\n",
        "    df_addresses = pd.DataFrame(addresses)\n",
        "    df_birth_dates = pd.DataFrame(birth_dates)\n",
        "    df_id_numbers = pd.DataFrame(id_numbers)\n",
        "    df_names = pd.DataFrame(names)\n",
        "\n",
        "    # 將 DataFrame 存入 Excel 的不同分頁\n",
        "    output_excel_path = \"parsed_results.xlsx\"\n",
        "    with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        df_addresses.to_excel(writer, sheet_name=\"地址\", index=False)\n",
        "        df_birth_dates.to_excel(writer, sheet_name=\"出生年月日\", index=False)\n",
        "        df_id_numbers.to_excel(writer, sheet_name=\"身分證字號\", index=False)\n",
        "        df_names.to_excel(writer, sheet_name=\"名字\", index=False)\n",
        "\n",
        "    print(f\"結果已存入 {output_excel_path}\")\n",
        "\n",
        "# 假設 document_object 是 Document AI 的解析結果\n",
        "# main(document_object, \"example_file.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzTRACy6A0oN"
      },
      "source": [
        "## Batch Processing Request to Document AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV_ZQlnELzZG",
        "outputId": "ca44ed9e-5acb-4560-d176-85510d3b85fe"
      },
      "outputs": [
        {
          "ename": "RetryError",
          "evalue": "Timeout of 120.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:74.125.23.95:443: tcp handshaker shutdown",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_call(\n\u001b[0;32m    278\u001b[0m         request,\n\u001b[0;32m    279\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    280\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    281\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    282\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mwait_for_ready,\n\u001b[0;32m    283\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\grpc\\_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[0;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[0;32m    331\u001b[0m )\n\u001b[1;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\grpc\\_channel.py:440\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thunk(new_method)\u001b[38;5;241m.\u001b[39mwith_call(\n\u001b[0;32m    316\u001b[0m         request,\n\u001b[0;32m    317\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mnew_timeout,\n\u001b[0;32m    318\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mnew_metadata,\n\u001b[0;32m    319\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mnew_credentials,\n\u001b[0;32m    320\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mnew_wait_for_ready,\n\u001b[0;32m    321\u001b[0m         compression\u001b[38;5;241m=\u001b[39mnew_compression,\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\grpc\\_channel.py:1198\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1192\u001b[0m (\n\u001b[0;32m   1193\u001b[0m     state,\n\u001b[0;32m   1194\u001b[0m     call,\n\u001b[0;32m   1195\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1196\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1197\u001b[0m )\n\u001b[1;32m-> 1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\grpc\\_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
            "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:74.125.23.95:443: tcp handshaker shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:74.125.23.95:443: tcp handshaker shutdown\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[1;31mServiceUnavailable\u001b[0m: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:74.125.23.95:443: tcp handshaker shutdown",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 154\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;66;03m# For a full list of Document object attributes, please reference this page:\u001b[39;00m\n\u001b[0;32m    146\u001b[0m             \u001b[38;5;66;03m# https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.types.Document\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \n\u001b[0;32m    148\u001b[0m             \u001b[38;5;66;03m# Read the text recognition output from the processor\u001b[39;00m\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;66;03m# print(\"The document contains the following text:\")\u001b[39;00m\n\u001b[0;32m    150\u001b[0m             \u001b[38;5;66;03m# print(document.text)\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     batch_process_documents(\n\u001b[0;32m    155\u001b[0m         project_id\u001b[38;5;241m=\u001b[39mPROJECT_ID,\n\u001b[0;32m    156\u001b[0m         location\u001b[38;5;241m=\u001b[39mLOCATION,\n\u001b[0;32m    157\u001b[0m         processor_id\u001b[38;5;241m=\u001b[39mPROCESSOR_ID,\n\u001b[0;32m    158\u001b[0m         gcs_input_uri\u001b[38;5;241m=\u001b[39mGCS_INPUT_URL,\n\u001b[0;32m    159\u001b[0m         gcs_output_uri\u001b[38;5;241m=\u001b[39mGCS_OUTPUT_URL,\n\u001b[0;32m    160\u001b[0m         input_mime_type\u001b[38;5;241m=\u001b[39minput_mime_type,\n\u001b[0;32m    161\u001b[0m         field_mask\u001b[38;5;241m=\u001b[39mfield_mask,\n\u001b[0;32m    162\u001b[0m     )\n",
            "Cell \u001b[1;32mIn[5], line 84\u001b[0m, in \u001b[0;36mbatch_process_documents\u001b[1;34m(project_id, location, processor_id, gcs_input_uri, gcs_output_uri, processor_version_id, input_mime_type, field_mask, timeout)\u001b[0m\n\u001b[0;32m     77\u001b[0m request \u001b[38;5;241m=\u001b[39m documentai\u001b[38;5;241m.\u001b[39mBatchProcessRequest(\n\u001b[0;32m     78\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m     79\u001b[0m     input_documents\u001b[38;5;241m=\u001b[39minput_config,\n\u001b[0;32m     80\u001b[0m     document_output_config\u001b[38;5;241m=\u001b[39moutput_config,\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# BatchProcess returns a Long Running Operation (LRO)\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m operation \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbatch_process_documents(request)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Continually polls the operation until it is complete.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# This could take some time for larger files\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Format: projects/{project_id}/locations/{location}/operations/{operation_id}\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\cloud\\documentai_v1\\services\\document_processor_service\\client.py:1073\u001b[0m, in \u001b[0;36mDocumentProcessorServiceClient.batch_process_documents\u001b[1;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1073\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m   1074\u001b[0m     request,\n\u001b[0;32m   1075\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m   1076\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1077\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   1078\u001b[0m )\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Wrap the response in an operation future.\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m response \u001b[38;5;241m=\u001b[39m operation\u001b[38;5;241m.\u001b[39mfrom_gapic(\n\u001b[0;32m   1082\u001b[0m     response,\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39moperations_client,\n\u001b[0;32m   1084\u001b[0m     document_processor_service\u001b[38;5;241m.\u001b[39mBatchProcessResponse,\n\u001b[0;32m   1085\u001b[0m     metadata_type\u001b[38;5;241m=\u001b[39mdocument_processor_service\u001b[38;5;241m.\u001b[39mBatchProcessMetadata,\n\u001b[0;32m   1086\u001b[0m )\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    295\u001b[0m     target,\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    297\u001b[0m     sleep_generator,\n\u001b[0;32m    298\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    299\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    300\u001b[0m )\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m _retry_error_helper(\n\u001b[0;32m    157\u001b[0m         exc,\n\u001b[0;32m    158\u001b[0m         deadline,\n\u001b[0;32m    159\u001b[0m         sleep_iter,\n\u001b[0;32m    160\u001b[0m         error_list,\n\u001b[0;32m    161\u001b[0m         predicate,\n\u001b[0;32m    162\u001b[0m         on_error,\n\u001b[0;32m    163\u001b[0m         exception_factory,\n\u001b[0;32m    164\u001b[0m         timeout,\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
            "File \u001b[1;32md:\\Miniconda\\envs\\rag\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:229\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m next_sleep \u001b[38;5;241m>\u001b[39m deadline:\n\u001b[0;32m    224\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    225\u001b[0m         error_list,\n\u001b[0;32m    226\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mTIMEOUT,\n\u001b[0;32m    227\u001b[0m         original_timeout,\n\u001b[0;32m    228\u001b[0m     )\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    230\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(error_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], next_sleep)\n\u001b[0;32m    232\u001b[0m )\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m next_sleep\n",
            "\u001b[1;31mRetryError\u001b[0m: Timeout of 120.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:74.125.23.95:443: tcp handshaker shutdown"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Makes a Batch Processing Request to Document AI\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.api_core.exceptions import InternalServerError\n",
        "from google.api_core.exceptions import RetryError\n",
        "from google.cloud import documentai\n",
        "from google.cloud import storage\n",
        "\n",
        "# TODO(developer): Fill these variables before running the sample.\n",
        "\n",
        "project_id = PROJECT_ID\n",
        "location = \"us\"  # Format is \"us\" or \"eu\"\n",
        "processor_id = PROCESSOR_ID  # Create processor before running sample\n",
        "gcs_output_uri = GCS_OUTPUT_URL  # Must end with a trailing slash `/`. Format: gs://bucket/directory/subdirectory/\n",
        "# processor_version_id = (\n",
        "#     \"YOUR_PROCESSOR_VERSION_ID\"  # Optional. Example: pretrained-ocr-v1.0-2020-09-23\n",
        "# )\n",
        "\n",
        "# TODO(developer): If `gcs_input_uri` is a single file, `mime_type` must be specified.\n",
        "gcs_input_uri = GCS_INPUT_URL  # Format: `gs://bucket/directory/file.pdf` or `gs://bucket/directory/`\n",
        "input_mime_type = \"application/pdf\"\n",
        "field_mask = \"text,entities,pages.pageNumber\"  # Optional. The fields to return in the Document object.\n",
        "\n",
        "\n",
        "def batch_process_documents(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    processor_id: str,\n",
        "    gcs_input_uri: str,\n",
        "    gcs_output_uri: str,\n",
        "    processor_version_id: str = None,\n",
        "    input_mime_type: str = None,\n",
        "    field_mask: str = None,\n",
        "    timeout: int = 400,\n",
        "):\n",
        "    # You must set the api_endpoint if you use a location other than \"us\".\n",
        "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
        "\n",
        "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
        "\n",
        "    if not gcs_input_uri.endswith(\"/\") and \".\" in gcs_input_uri:\n",
        "        # Specify specific GCS URIs to process individual documents\n",
        "        gcs_document = documentai.GcsDocument(\n",
        "            gcs_uri=gcs_input_uri, mime_type=input_mime_type\n",
        "        )\n",
        "        # Load GCS Input URI into a List of document files\n",
        "        gcs_documents = documentai.GcsDocuments(documents=[gcs_document])\n",
        "        input_config = documentai.BatchDocumentsInputConfig(gcs_documents=gcs_documents)\n",
        "    else:\n",
        "        # Specify a GCS URI Prefix to process an entire directory\n",
        "        gcs_prefix = documentai.GcsPrefix(gcs_uri_prefix=gcs_input_uri)\n",
        "        input_config = documentai.BatchDocumentsInputConfig(gcs_prefix=gcs_prefix)\n",
        "\n",
        "    # Cloud Storage URI for the Output Directory\n",
        "    gcs_output_config = documentai.DocumentOutputConfig.GcsOutputConfig(\n",
        "        gcs_uri=gcs_output_uri, field_mask=field_mask\n",
        "    )\n",
        "\n",
        "    # Where to write results\n",
        "    output_config = documentai.DocumentOutputConfig(gcs_output_config=gcs_output_config)\n",
        "\n",
        "    if processor_version_id:\n",
        "        # The full resource name of the processor version, e.g.:\n",
        "        # projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}\n",
        "        name = client.processor_version_path(\n",
        "            project_id, location, processor_id, processor_version_id\n",
        "        )\n",
        "    else:\n",
        "        # The full resource name of the processor, e.g.:\n",
        "        # projects/{project_id}/locations/{location}/processors/{processor_id}\n",
        "        name = client.processor_path(project_id, location, processor_id)\n",
        "\n",
        "    request = documentai.BatchProcessRequest(\n",
        "        name=name,\n",
        "        input_documents=input_config,\n",
        "        document_output_config=output_config,\n",
        "    )\n",
        "\n",
        "    # BatchProcess returns a Long Running Operation (LRO)\n",
        "    operation = client.batch_process_documents(request)\n",
        "\n",
        "    # Continually polls the operation until it is complete.\n",
        "    # This could take some time for larger files\n",
        "    # Format: projects/{project_id}/locations/{location}/operations/{operation_id}\n",
        "    try:\n",
        "        print(f\"Waiting for operation {operation.operation.name} to complete...\")\n",
        "        operation.result(timeout=timeout)\n",
        "    # Catch exception when operation doesn\"t finish before timeout\n",
        "    except (RetryError, InternalServerError) as e:\n",
        "        print(e.message)\n",
        "\n",
        "    # NOTE: Can also use callbacks for asynchronous processing\n",
        "    #\n",
        "    # def my_callback(future):\n",
        "    #   result = future.result()\n",
        "    #\n",
        "    # operation.add_done_callback(my_callback)\n",
        "\n",
        "    # Once the operation is complete,\n",
        "    # get output document information from operation metadata\n",
        "    metadata = documentai.BatchProcessMetadata(operation.metadata)\n",
        "\n",
        "    if metadata.state != documentai.BatchProcessMetadata.State.SUCCEEDED:\n",
        "        raise ValueError(f\"Batch Process Failed: {metadata.state_message}\")\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    print(\"Output files:\")\n",
        "    # One process per Input Document\n",
        "    for process in list(metadata.individual_process_statuses):\n",
        "        # output_gcs_destination format: gs://BUCKET/PREFIX/OPERATION_NUMBER/INPUT_FILE_NUMBER/\n",
        "        # The Cloud Storage API requires the bucket name and URI prefix separately\n",
        "        matches = re.match(r\"gs://(.*?)/(.*)\", process.output_gcs_destination)\n",
        "        if not matches:\n",
        "            print(\n",
        "                \"Could not parse output GCS destination:\",\n",
        "                process.output_gcs_destination,\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        output_bucket, output_prefix = matches.groups()\n",
        "\n",
        "        # Get List of Document Objects from the Output Bucket\n",
        "        output_blobs = storage_client.list_blobs(output_bucket, prefix=output_prefix)\n",
        "\n",
        "        # Document AI may output multiple JSON files per source file\n",
        "        for blob in output_blobs:\n",
        "            # Document AI should only output JSON files to GCS\n",
        "            if blob.content_type != \"application/json\":\n",
        "                print(\n",
        "                    f\"Skipping non-supported file: {blob.name} - Mimetype: {blob.content_type}\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            # Download JSON File as bytes object and convert to Document Object\n",
        "            print(f\"Fetching {blob.name}\")\n",
        "            document = documentai.Document.from_json(\n",
        "                blob.download_as_bytes(), ignore_unknown_fields=True\n",
        "            )\n",
        "\n",
        "            # For a full list of Document object attributes, please reference this page:\n",
        "            # https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.types.Document\n",
        "\n",
        "            # Read the text recognition output from the processor\n",
        "            # print(\"The document contains the following text:\")\n",
        "            # print(document.text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_process_documents(\n",
        "        project_id=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        processor_id=PROCESSOR_ID,\n",
        "        gcs_input_uri=GCS_INPUT_URL,\n",
        "        gcs_output_uri=GCS_OUTPUT_URL,\n",
        "        input_mime_type=input_mime_type,\n",
        "        field_mask=field_mask,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znaPhHQ3XCWa"
      },
      "source": [
        "### 步驟 1：處理批量處理的結果\n",
        "\n",
        "batch_process_documents() 方法的輸出結果會存儲在 Google Cloud Storage 中，通常是 JSON 格式的文件。您需要下載這些 JSON 文件，然後對每個文件進行資料萃取。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psYurbCnXIt8"
      },
      "source": [
        "### 步驟 2：下載批量處理的結果\n",
        "使用 Google Cloud Storage 的 Python 客戶端庫下載處理結果：\n",
        "\n",
        "下載結果的程式碼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1LXZXNsXBEK",
        "outputId": "49e56de5-93df-40f7-e762-7d5fc7d878b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "下載文件: results/15560794818710564337/0/測試用連署書-0.json 到 ./batch_results\\測試用連署書-0.json\n",
            "下載文件: results/15560794818710564337/1/測試用連署書_手寫-0.json 到 ./batch_results\\測試用連署書_手寫-0.json\n",
            "下載文件: results/15560794818710564337/2/測試用連署書_打字3人-0.json 到 ./batch_results\\測試用連署書_打字3人-0.json\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "import os\n",
        "import json\n",
        "\n",
        "def download_batch_results(bucket_name, output_prefix, local_output_dir):\n",
        "    \"\"\"下載批量處理的結果到本地\"\"\"\n",
        "    # 初始化 GCS 客戶端\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # 列出所有輸出文件\n",
        "    blobs = bucket.list_blobs(prefix=output_prefix)\n",
        "    os.makedirs(local_output_dir, exist_ok=True)\n",
        "\n",
        "    downloaded_files = []\n",
        "    for blob in blobs:\n",
        "        # 只下載 JSON 文件\n",
        "        if blob.name.endswith(\".json\"):\n",
        "            local_file_path = os.path.join(local_output_dir, os.path.basename(blob.name))\n",
        "            blob.download_to_filename(local_file_path)\n",
        "            downloaded_files.append(local_file_path)\n",
        "            print(f\"下載文件: {blob.name} 到 {local_file_path}\")\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "# 使用範例\n",
        "bucket_name = BUCKET_NAME                          #Google Storage上的Bucket Name\n",
        "output_prefix = \"results/15560794818710564337/\"  # 批量處理的輸出目錄 #從Google Storage上選擇\n",
        "local_output_dir = \"./batch_results\"             # 本地存儲目錄\n",
        "\n",
        "downloaded_files = download_batch_results(bucket_name, output_prefix, local_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP2n7sydXQls"
      },
      "source": [
        "### 步驟 3：解析批量處理的結果\n",
        "每個 JSON 文件代表一個處理結果，您可以使用之前的 parse_document() 函式來解析這些結果。\n",
        "\n",
        "解析 JSON 文件的程式碼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5lcW1u7XTQb",
        "outputId": "be340791-783e-4314-d520-de4af5ed639e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "\n",
        "\n",
        "def parse_document_from_json(json_file_path, file_name):\n",
        "    \"\"\"從 JSON 文件中解析文檔內容，並判定頁碼\"\"\"\n",
        "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        document_data = json.load(f)\n",
        "\n",
        "    # 獲取文檔的文字內容\n",
        "    document_text = document_data.get(\"text\", \"\")\n",
        "    # 將文字內容按行分割\n",
        "    lines = document_text.split(\"\\n\")\n",
        "\n",
        "    # 初始化結果列表\n",
        "    addresses = []\n",
        "    birth_dates = []\n",
        "    id_numbers = []\n",
        "    names = []\n",
        "\n",
        "    # 初始化頁碼\n",
        "    current_page = 0\n",
        "    noise_words = [\n",
        "        \"地址\", \"先生\", \"小姐\", \"姓名\",\n",
        "        \"性別\", \"年齡\", \"民國\", \"新北市\", \"台北市\", \"中正區\",\n",
        "        \"罷免案\", \"連署人\", \"名冊\", \"國統\", \"樓之\",\"出生年\",\"聯絡\",\"電話\",\"行政區\",\"編姓\"\n",
        "    ]\n",
        "    # 遍歷每一行\n",
        "    for line_number, line_text in enumerate(lines, start=1):\n",
        "        line_text = line_text.strip()\n",
        "        if not line_text:\n",
        "            continue  # 跳過空行\n",
        "\n",
        "        # 判定是否是新的一頁\n",
        "        if \"罷免案連署人名冊\" in line_text:\n",
        "            current_page += 1  # 遇到標題時，頁碼加 1\n",
        "            continue  # 跳過標題行\n",
        "\n",
        "        # 初始化當前行的資料\n",
        "        current_data = {\n",
        "            \"地址\": None,\n",
        "            \"里\": None,\n",
        "            \"鄰\": None,\n",
        "            \"路/街\": None,\n",
        "            \"出生年月日\": None,\n",
        "            \"年齡\": None,\n",
        "            \"身分證字號\": None,\n",
        "            \"性別\": None,\n",
        "            \"行號\": line_number,\n",
        "            \"頁碼\": current_page\n",
        "        }\n",
        "\n",
        "        # 提取名字\n",
        "\n",
        "        # 提取名字\n",
        "        if re.match(r\"^[\\u4e00-\\u9fa5]{2,3}$\", line_text) and line_text not in noise_words:\n",
        "            names.append({\n",
        "                \"檔案名稱\": file_name,\n",
        "                \"頁碼\": current_page,\n",
        "                \"姓名\": line_text\n",
        "            })\n",
        "\n",
        "        # 提取地址\n",
        "        if \"里\" in line_text and \"鄰\" in line_text:\n",
        "            current_data[\"地址\"] = line_text\n",
        "            current_data[\"里\"] = extract_specific_field(line_text, r\"([一-龥]{1,2})里\")\n",
        "            current_data[\"鄰\"] = extract_specific_field(line_text, r\"(\\d{1,2})鄰\")\n",
        "            current_data[\"路/街\"] = extract_specific_field(line_text, r\"([一-龥]{1,3})(路|街)(?!鄰)\")\n",
        "\n",
        "        # 提取出生年月日\n",
        "        birth_date = extract_birth_date(line_text)\n",
        "        if birth_date:\n",
        "            current_data[\"出生年月日\"] = birth_date\n",
        "            current_data[\"年齡\"] = calculate_age(birth_date)\n",
        "\n",
        "        # 提取身分證字號\n",
        "        if re.search(r\"[A-Z][0-9]+\", line_text):\n",
        "            current_data[\"身分證字號\"] = line_text\n",
        "            current_data[\"性別\"] = determine_gender(line_text)\n",
        "\n",
        "        # 如果該行有地址，存入地址列表\n",
        "        if current_data[\"地址\"]:\n",
        "            addresses.append({\n",
        "                \"地址\": current_data[\"地址\"],\n",
        "                \"里\": current_data[\"里\"],\n",
        "                \"鄰\": current_data[\"鄰\"],\n",
        "                \"路/街\": current_data[\"路/街\"]\n",
        "            })\n",
        "\n",
        "        # 如果該行有出生年月日，存入出生年月日列表\n",
        "        if current_data[\"出生年月日\"]:\n",
        "            birth_dates.append({\n",
        "                \"出生年月日\": current_data[\"出生年月日\"],\n",
        "                \"年齡\": current_data[\"年齡\"]\n",
        "            })\n",
        "\n",
        "        # 如果該行有身分證字號，存入身分證字號列表\n",
        "        if current_data[\"身分證字號\"]:\n",
        "            id_numbers.append({\n",
        "                \"身分證字號前五碼\": current_data[\"身分證字號\"][:5],\n",
        "                \"性別\": current_data[\"性別\"]\n",
        "            })\n",
        "\n",
        "    return addresses, birth_dates, id_numbers, names\n",
        "\n",
        "\n",
        "def extract_specific_field(text, pattern):\n",
        "    \"\"\"提取地址中的特定字段（如里、鄰、路/街）\"\"\"\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        result = match.group(0)\n",
        "        return result.replace(\"鄰\", \"\")  # 確保「鄰」字被移除\n",
        "    return None\n",
        "\n",
        "def extract_birth_date(text):\n",
        "    \"\"\"提取生日，支持多種格式並修正錯誤年份\"\"\"\n",
        "    match = re.search(r\"(\\d{2,3}\\.\\d{1,2}\\.\\d{1,2})|民國(\\d{2,3})年(\\d{1,2})月(\\d{1,2})日|(\\d{2,3})年(\\d{1,2})月(\\d{1,2})日\", text)\n",
        "    if match:\n",
        "        if match.group(1):  # 簡寫格式\n",
        "            year, month, day = match.group(1).split(\".\")\n",
        "            year = correct_year(year)\n",
        "            return f\"{year}.{month}.{day}\"\n",
        "        elif match.group(2):  # 民國完整格式\n",
        "            year = correct_year(match.group(2))\n",
        "            month = match.group(3)\n",
        "            day = match.group(4)\n",
        "            return f\"{year}.{month}.{day}\"\n",
        "        elif match.group(5):  # 未標註民國的格式\n",
        "            year = correct_year(match.group(5))\n",
        "            month = match.group(6)\n",
        "            day = match.group(7)\n",
        "            return f\"{year}.{month}.{day}\"\n",
        "    return None\n",
        "\n",
        "def correct_year(year):\n",
        "    \"\"\"修正年份，確保符合規則\"\"\"\n",
        "    year = int(year)\n",
        "    if len(str(year)) == 3 and str(year)[0] != \"1\":\n",
        "        year = int(str(year)[-2:])\n",
        "    return year\n",
        "\n",
        "def calculate_age(birth_date):\n",
        "    \"\"\"根據出生年月日計算年齡\"\"\"\n",
        "    try:\n",
        "        if re.match(r\"^\\d{2,3}\\.\\d{1,2}\\.\\d{1,2}$\", birth_date):\n",
        "            year, month, day = map(int, birth_date.split(\".\"))\n",
        "            year += 1911  # 民國轉西元\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        birth_date = datetime(year, month, day)\n",
        "        today = datetime.today()\n",
        "        age = today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day))\n",
        "        return age\n",
        "    except Exception as e:\n",
        "        print(f\"無法計算年齡: {e}\")\n",
        "        return None\n",
        "\n",
        "def determine_gender(id_number):\n",
        "    \"\"\"根據身分證字號判斷性別\"\"\"\n",
        "    try:\n",
        "        match = re.search(r\"[A-Z]([0-9])\", id_number)\n",
        "        if match:\n",
        "            gender_digit = int(match.group(1))\n",
        "            return \"男性\" if gender_digit == 1 else \"女性\"\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"無法判斷性別: {e}\")\n",
        "        return None\n",
        "\n",
        "# 主程式\n",
        "def main(json_files, output_excel_path):\n",
        "    os.makedirs(output_excel_path, exist_ok=True)\n",
        "    all_addresses = []\n",
        "    all_birth_dates = []\n",
        "    all_id_numbers = []\n",
        "    all_names = []\n",
        "\n",
        "    for json_file in json_files:\n",
        "        file_name = os.path.basename(json_file)  # 提取檔案名稱\n",
        "        print(f\"處理文件: {json_file}\")\n",
        "        addresses, birth_dates, id_numbers, names = parse_document_from_json(json_file, file_name)\n",
        "        all_addresses.extend(addresses)\n",
        "        all_birth_dates.extend(birth_dates)\n",
        "        all_id_numbers.extend(id_numbers)\n",
        "        all_names.extend(names)\n",
        "\n",
        "    # 將結果存入 DataFrame\n",
        "    df_addresses = pd.DataFrame(all_addresses)\n",
        "    df_birth_dates = pd.DataFrame(all_birth_dates)\n",
        "    df_id_numbers = pd.DataFrame(all_id_numbers)\n",
        "    df_names = pd.DataFrame(all_names)\n",
        "    current_time = datetime.now()\n",
        "    # 將 DataFrame 存入 Excel 的不同分頁\n",
        "    with pd.ExcelWriter(f\"{output_excel_path}/parse_{current_time.strftime('%m_%d_%H_%M')}.xlsx\", engine=\"openpyxl\") as writer:\n",
        "        df_addresses.to_excel(writer, sheet_name=\"地址\", index=False)\n",
        "        df_birth_dates.to_excel(writer, sheet_name=\"出生年月日\", index=False)\n",
        "        df_id_numbers.to_excel(writer, sheet_name=\"身分證字號\", index=False)\n",
        "        df_names.to_excel(writer, sheet_name=\"名字\", index=False)\n",
        "    \n",
        "    print(f\"結果已存入 {output_excel_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXkdsU4fXWEy"
      },
      "source": [
        "### 步驟 4：存成 CSV 文件\n",
        "將合併後的 DataFrame 存成 CSV 文件：\n",
        "\n",
        "存成 CSV 的程式碼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qqyKeikmXYi7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "處理文件: ./batch_results\\測試用連署書-0.json\n",
            "處理文件: ./batch_results\\測試用連署書_手寫-0.json\n",
            "處理文件: ./batch_results\\測試用連署書_打字3人-0.json\n",
            "結果已存入 parsed_results\n"
          ]
        }
      ],
      "source": [
        "# 取得當前的日期和時間\n",
        "\n",
        "\n",
        "# 格式化日期和時間為 \"月_日_時_分\"\n",
        "\n",
        "# 執行資料萃取並存入 Excel\n",
        "output_excel_dir = f\"parsed_results\"\n",
        "main(downloaded_files, output_excel_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
